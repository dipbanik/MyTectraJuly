{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h6ClJ7-it5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS6CIJ-Ti3M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize parameters\n",
        "gamma = 0.75 # Discount factor\n",
        "alpha = 0.9 # Learning rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc4BO2ITi8dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the states \n",
        "location_to_state = {\n",
        "    'L1' : 0 ,\n",
        "    'L2' : 1 ,\n",
        "    'L3' : 2 ,\n",
        "    'L4' : 3 ,\n",
        "    'L5' : 4 ,\n",
        "    'L6' : 5 ,\n",
        "    'L7' : 6 ,\n",
        "    'L8' : 7 ,\n",
        "    'L9' : 8\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uX9CQ7GjdsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the actions\n",
        "actions = [0,1,2,3,4,5,6,7,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIHLEbHBjmud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rewards\n",
        "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
        "                    [1,0,1,0,0,0,0,0,0],\n",
        "                    [0,1,0,0,0,1,0,0,0],\n",
        "                    [0,0,0,0,0,0,1,0,0],\n",
        "                    [0,1,0,0,0,0,0,1,0],\n",
        "                    [0,0,1,0,0,0,0,0,0],\n",
        "                    [0,0,0,1,0,0,0,1,0],\n",
        "                    [0,0,0,0,1,0,1,0,1],\n",
        "                    [0,0,0,0,0,0,0,1,0]\n",
        "                    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzl_gVw8kLNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_to_location = dict((state,location) for location,state in location_to_state.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9cypRn4k0j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimal_route(start_location, end_location):\n",
        "  #copy the rewards matrix to new matrix\n",
        "  rewards_new = np.copy(rewards)\n",
        "  #get the ending state corresponding to the ending location as given\n",
        "  ending_state = location_to_state[end_location]\n",
        "  #With the above information automatically set the priority of the given ending state to the Highest one\n",
        "  rewards_new[ending_state, ending_state] = 999\n",
        "\n",
        "  # Q-Learning algo\n",
        "\n",
        "  #Initializing Q-values\n",
        "  Q = np.array(np.zeros([9,9]))\n",
        "\n",
        "  # Q-Learning process\n",
        "  for i in range(1000):\n",
        "    #Pick up a state randomly\n",
        "    current_state = np.random.randint(0,9) # Python excludes this upper bound\n",
        "    # for traversing through the neighbor locations in the maze\n",
        "    playable_actions = []\n",
        "    #Iterate through the new rewards matrix and get the actions > 0\n",
        "    for j in range(9):\n",
        "      if rewards_new[current_state,j] > 0 :\n",
        "        playable_actions.append(j)\n",
        "    #Pick an action randomly for the list of playable actions leading toi the next state\n",
        "    next_state = np.random.choice(playable_actions)\n",
        "    #Compyte the temporaral difference\n",
        "    #The action here exactly refers to going to othe next state\n",
        "    TD = rewards_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n",
        "#########################################                                                                                                       ######################################################]\n",
        "    # Update the Q value using the bellman equation\n",
        "    Q[current_state, next_state] += alpha * TD\n",
        "  \n",
        "  #Initialize the optimal route with the starting location\n",
        "  route = [start_location]\n",
        "  # We do not know about the next location yet, so initialize with the value of the start location.\n",
        "  next_location = start_location\n",
        "\n",
        "  #We don't know about the exact number of iterations needed to reach the final location hence while loop will be a good choice for the iteration\n",
        "  while(next_location != end_location):\n",
        "    #Fetch the starting state\n",
        "    starting_state  = location_to_state[start_location]\n",
        "    #Fetch the highest Q-value pertaining to starting state\n",
        "    next_state = np.argmax(Q[starting_state,])\n",
        "    #we go the index of the next state. But we need the corresponding letter.\n",
        "    next_location = state_to_location[next_state]\n",
        "    route.append(next_location)\n",
        "    #Update the starting locatopn for the next iteration\n",
        "    start_location = next_location\n",
        "\n",
        "  return route\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVjPH2X3q0VD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "62de1eb0-9714-4924-9563-7e143b72594a"
      },
      "source": [
        "print(get_optimal_route('L1', 'L9'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ba6f7d38dabf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_optimal_route\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L9'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-8193e6333076>\u001b[0m in \u001b[0;36mget_optimal_route\u001b[0;34m(start_location, end_location)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mstarting_state\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlocation_to_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_location\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#Fetch the highest Q-value pertaining to starting state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstarting_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m#we go the index of the next state. But we need the corresponding letter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mnext_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_to_location\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RbER5_htkFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}